{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training the first neural net\n",
    "\n",
    "\n",
    "## Contents\n",
    "- Setting up a TensorFlow instance\n",
    "- Downloading the Iris dataset which contains 4 physiological labels of 3 different kinds of Iris flowers: setosa, versicolor, and virginica\n",
    "- Reshaping the data in order to feed them to a NN\n",
    "- Creating a NN of a certain shape\n",
    "- Train the NN on the training data using a loss function\n",
    "- Test the NN on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The cell above contains some \"magic\": it imports certain libraries that allow to use OS functionalities, plotting and most importantly the TensorFlow library.\n",
    "\n",
    "Now we download the dataset. We could do this by hand, but since this is a standard data set, TensorFlow actually offers it on their website. Using keras and its utilities we can download and store the path to the downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname = os.path.basename(train_dataset_url), origin=train_dataset_url)\n",
    "\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The file is in .csv format, which you can open with Excel or Google Sheets. We take a quick peek here to get an idea what the file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!head -n5 {train_dataset_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the first line the number of entries in the list; the number of measurements for each entry; the names of the three possible labels. The second and further lines show the 4 measurements and a label 0, 1, or 2 corresponding to the label names in the first row.\n",
    "\n",
    "We whish to teach the machine to predict the label from the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(line):\n",
    "    example_defaults = [[0.],[0.],[0.],[0.],[0]]\n",
    "    parsed_line = tf.decode_csv(line, example_defaults)\n",
    "    features = tf.reshape(parsed_line[:-1], shape=(4,))\n",
    "    label = tf.reshape(parsed_line[-1], shape=())\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we bring the data into a format that we can use for tensorflow.\n",
    "\n",
    "And now we are going to prepare the test dataset by shuffling it, putting it into batches and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TextLineDataset(train_dataset_fp)\n",
    "train_dataset = train_dataset.skip(1)\n",
    "train_dataset = train_dataset.map(parse_csv)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 1000)\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "features, label = iter(train_dataset).next()\n",
    "print(\"example features:\", features[0])\n",
    "print(\"example label:\", label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the magic: we build the neural network. It consists of a layer of 10 neurons, another layer of 10 neurons and a last output layer with 3 nodes --> the 3 different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "#add layers here :)\n",
    "#In the following examples your main task will be to set up the network. \n",
    "#I will give a sample, and you can check https://www.tensorflow.org/api_docs/python/tf/layers\n",
    "#for all the possible layers and ways they can be connected.\n",
    "         \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the error, or loss, function and the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the training rate to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer here\n",
    "# And at http://tflearn.org/optimizers you can take a look at all the different optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: the big show.\n",
    "\n",
    "We loop through 200 epochs of comparing our predictions to the training samples and updating the weights according to the loss function. We print out loss and accuracy for every 50th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "# set num_epochs = something here\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tfe.metrics.Mean()\n",
    "    epoch_accuracy = tfe.metrics.Accuracy()\n",
    "    \n",
    "    for x, y in train_dataset:\n",
    "        grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), global_step = tf.train.get_or_create_global_step())\n",
    "        epoch_loss_avg(loss(model, x, y))\n",
    "        epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n",
    "        \n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little plot of loss and accuracy: We see that the loss (\"error\") decreases to about 10% and the accuracy up to somewhere between 95% and 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12,8))\n",
    "fig.suptitle(\"Training Metrics\")\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\",fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model, we download some further sample data. They were not part of the test data, so they're unknown to our model. We check its predictions and find a pretty good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url), origin = test_url)\n",
    "\n",
    "test_dataset = tf.data.TextLineDataset(test_fp)\n",
    "test_dataset = test_dataset.skip(1)\n",
    "test_dataset = test_dataset.map(parse_csv)\n",
    "test_dataset = test_dataset.shuffle(1000)\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "for (x, y) in test_dataset:\n",
    "    prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n",
    "    test_accuracy(prediction, y)\n",
    "    \n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
